{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhaoyiming/Desktop/ETHZurich/1. Y1S1/2. AML/Project/task-2-ECG-Signals-Classification/Models\n",
      "/Users/zhaoyiming/Desktop/ETHZurich/1. Y1S1/2. AML/Project/task-2-ECG-Signals-Classification/FeaturePreprocessing\n",
      "/Users/zhaoyiming/Desktop/ETHZurich/1. Y1S1/2. AML/Project/task-2-ECG-Signals-Classification/Models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# try usual models and select the best one\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    VotingClassifier,\n",
    "    BaggingClassifier,\n",
    "    StackingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    RidgeClassifier,\n",
    "    SGDClassifier,\n",
    "    PassiveAggressiveClassifier,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "sys.path.append(current_dir)\n",
    "from MODELS import finetuned_models as models\n",
    "from MODELS import vanilla_models as vanilla_models\n",
    "sys.path.append(current_dir + \"/../\")\n",
    "sys.path.append(current_dir + \"/../FeaturePreprocessing/\")\n",
    "\n",
    "from utils import *\n",
    "from FeaturePreprocessing.process import *\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Time taken by load_final_data: 0.43 seconds%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "(5117, 308) (3411, 308) (5117,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train = load_final_data()\n",
    "print(X_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LGBM|0.8760979884530793\\n', 'XGB|0.8741431451612904\\n', 'CatBoost|0.875\\n', 'GradientBoosting|0.8682\\n', 'HistGradientBoosting|0.8721\\n', 'ExtraTrees|0.8672\\n', 'RandomForest|0.8662\\n']\n",
      "['LGBM', 'XGB', 'CatBoost', 'GradientBoosting', 'HistGradientBoosting', 'ExtraTrees', 'RandomForest']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(current_dir, \"selected_models.txt\"), \"r\") as f:\n",
    "    selected_models = f.readlines()\n",
    "\n",
    "print(selected_models)\n",
    "selected_models = [model.split(\"|\")[0] for model in selected_models]\n",
    "print(selected_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    3030\n",
       "2    1474\n",
       "1     443\n",
       "3     170\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['LGBM', 'XGB', 'CatBoost', 'GradientBoosting', 'ExtraTrees', 'Bagging', 'RandomForest', 'HistGradientBoosting']\n",
    "# fine tune each model\n",
    "weight = {\n",
    "    0: 2,\n",
    "    1: 1,\n",
    "    2: 1.5,\n",
    "    3: 1,\n",
    "}\n",
    "LGBM_params = {\n",
    "    # \"num_leaves\": [10, 20, 30, 40, 50],\n",
    "    \"max_depth\": [5],\n",
    "    \"learning_rate\": [0.07],\n",
    "    \"n_estimators\": [1000],\n",
    "    \"subsample\": [0.9],\n",
    "    \"class_weight\": [\"balanced\", None, weight],\n",
    "}\n",
    "\n",
    "lgbm_best = get_best_parameters(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    estimator=models[\"LGBM\"],\n",
    "    parameters=LGBM_params,\n",
    "    verbose=2,\n",
    ")\n",
    "# evaluate_model(X_train, y_train, models[\"LGBM\"], cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8741431451612904"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine tune XGB\n",
    "# XGB_params = {\n",
    "#     # \"max_depth\": [5],\n",
    "#     \"learning_rate\": np.linspace(0.01, 0.1, 10),\n",
    "#     \"n_estimators\": range(100, 1000, 100),\n",
    "#     # \"subsample\": [0.9],\n",
    "#     # \"colsample_bytree\": [0.9],\n",
    "# }\n",
    "\n",
    "# xgb_best = get_best_parameters(\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     estimator=models[\"XGB\"],\n",
    "#     parameters=XGB_params,\n",
    "#     verbose=2,\n",
    "# )\n",
    "\n",
    "evaluate_model(X_train, y_train, models[\"XGB\"], cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1313b2510>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CatBoostClassifier(auto_class_weights=\"Balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune CatBoost\n",
    "import catboost\n",
    "clf = catboost.CatBoostClassifier(auto_class_weights=\"Balanced\")\n",
    "\n",
    "CatBoost_params = {\n",
    "    # \"learning_rate\": np.linspace(0.01, 0.1, 10),\n",
    "    # \"n_estimators\": range(100, 1000, 100),\n",
    "    \"verbose\": [0],\n",
    "    \"auto_class_weights\": [\"SqrtBalanced\"],\n",
    "}\n",
    "\n",
    "catboost_best = get_best_parameters(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    estimator=clf,\n",
    "    parameters=CatBoost_params,\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching the best parameters:  50%|█████     | 1/2 [01:59<01:59, 119.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 0 params: (0, 'balanced') score: 0.8749 best: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching the best parameters: 100%|██████████| 2/2 [04:15<00:00, 127.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 1 params: (0, None) score: 0.8734 best: 0.8749\n",
      "Best params: {'verbose': 0, 'class_weight': 'balanced'}\n",
      "score: 0.8749257316104595\n",
      "best: HistGradientBoostingClassifier(learning_rate=0.01, max_iter=500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fine tune HistGradientBoosting\n",
    "HistGDBT_params = {\n",
    "    # \"learning_rate\": np.linspace(0.01, 0.1, 10),\n",
    "    # \"max_iter\": range(100, 1000, 100),\n",
    "    \"verbose\": [0],\n",
    "    \"class_weight\": [\"balanced\"],\n",
    "}\n",
    "\n",
    "histgdbt_best = get_best_parameters(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    estimator=models[\"HistGradientBoosting\"],\n",
    "    parameters=HistGDBT_params,\n",
    "    verbose=2,\n",
    "    cv=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching the best parameters:  33%|███▎      | 1/3 [00:20<00:41, 20.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 0 params: (500, 15, 'log_loss', 'balanced') score: 0.8724 best: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching the best parameters:  67%|██████▋   | 2/3 [00:41<00:20, 20.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 1 params: (500, 15, 'log_loss', 'balanced_subsample') score: 0.8734 best: 0.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching the best parameters: 100%|██████████| 3/3 [01:01<00:00, 20.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2 params: (500, 15, 'log_loss', None) score: 0.8722 best: 0.8734\n",
      "Best params: {'n_estimators': 500, 'max_depth': 15, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample'}\n",
      "score: 0.8733622770039101\n",
      "best: ExtraTreesClassifier(criterion='log_loss', max_depth=15, n_estimators=500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fine tune ExtraTrees\n",
    "ExtraTrees_params = {\n",
    "    \"n_estimators\": [500],\n",
    "    \"max_depth\": [15],\n",
    "    \"criterion\" : [\"log_loss\"],\n",
    "    \"class_weight\": [\"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "extratrees_best = get_best_parameters(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    estimator=models[\"ExtraTrees\"],\n",
    "    parameters=ExtraTrees_params,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune Bagging\n",
    "Bagging_params = {\n",
    "    \"n_estimators\": [10, 100, 200, 500],\n",
    "    # \"max_samples\": [0.9, 1.0],\n",
    "    # \"max_features\": [0.9, 1.0],\n",
    "    # \"bootstrap\": [True, False],\n",
    "    \n",
    "}\n",
    "\n",
    "bagging_best = get_best_parameters(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    estimator=models[\"Bagging\"],\n",
    "    parameters=Bagging_params,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching the best parameters:  50%|█████     | 1/2 [00:57<00:57, 57.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Time taken by evaluate_model: 57.71 seconds%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "No: 0 params: (700, 'sqrt') score: 0.8708 best: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching the best parameters: 100%|██████████| 2/2 [01:30<00:00, 45.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Time taken by evaluate_model: 32.48 seconds%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "No: 1 params: (700, 'log2') score: 0.8681 best: 0.8708\n",
      "Best params: {'n_estimators': 700, 'max_features': 'sqrt'}\n",
      "score: 0.8708228326612903\n",
      "best: RandomForestClassifier(class_weight='balanced_subsample', max_features='log2',\n",
      "                       n_estimators=700, n_jobs=-1, random_state=42)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%Time taken by get_best_parameters: 90.19 seconds%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fine tune RandomForest\n",
    "\n",
    "RandomForest_params = {\n",
    "    \"n_estimators\": [700],\n",
    "    # \"max_depth\": [5],\n",
    "    # \"min_samples_split\": [2],\n",
    "    # \"min_samples_leaf\": [1],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    # \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "randomforest_best = get_best_parameters(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    estimator=models[\"RandomForest\"],\n",
    "    parameters=RandomForest_params,\n",
    "    verbose=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
